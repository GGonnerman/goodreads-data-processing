{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34eb50c6-e36d-4caf-8c7d-146ca43700bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import csv\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import simplejson as json\n",
    "from faker import Faker\n",
    "from datetime import datetime\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d795d5dc-077d-4ee2-a21d-f88ac7b36fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_output_path = \"out\"\n",
    "chosen_book_ids_path = os.path.join(base_output_path, \"chosen_book_ids.pickle\")\n",
    "full_book_information_path = os.path.join(base_output_path, \"full_book_information.csv\")\n",
    "export_path = os.path.join(base_output_path, \"export\")\n",
    "book_path = os.path.join(export_path, \"book.csv\")\n",
    "book_has_author_path = os.path.join(export_path, \"book_has_author.csv\")\n",
    "book_similar_to_book_path = os.path.join(export_path, \"book_similar_to_book.csv\")\n",
    "author_path = os.path.join(export_path, \"author.csv\")\n",
    "review_path = os.path.join(export_path, \"review.csv\")\n",
    "user_has_read_book_path = os.path.join(export_path, \"user_has_read_book.csv\")\n",
    "user_owns_book_path = os.path.join(export_path, \"user_owns_book.csv\")\n",
    "user_path = os.path.join(export_path, \"user.csv\")\n",
    "# Sample files\n",
    "review_sample_path = os.path.join(export_path, \"review-sample.csv\")\n",
    "user_has_read_book_sample_path = os.path.join(export_path, \"user_has_read_book-sample.csv\")\n",
    "user_owns_book_sample_path = os.path.join(export_path, \"user_owns_book-sample.csv\")\n",
    "user_sample_path = os.path.join(export_path, \"user-sample.csv\")\n",
    "#\n",
    "\n",
    "Path(base_output_path).mkdir(parents=True, exist_ok=True)\n",
    "Path(export_path).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a397cdcf-355f-49f0-bfcb-16a36a54d624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished generating chosen_book_ids\n",
      "Saving...\n",
      "Finished saving\n"
     ]
    }
   ],
   "source": [
    "# Generate the list of book ids that we want to use\n",
    "work_json = os.path.join(\"meta_data\", \"goodreads_book_works.json\")\n",
    "\n",
    "try:\n",
    "    os.remove(chosen_book_ids_path)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "chosen_book_ids = set()\n",
    "\n",
    "for i, chunk in enumerate(pd.read_json(work_json, chunksize = 15000, lines=True)):\n",
    "    # Only save works with > 1000 reviews\n",
    "    chunk = chunk[pd.to_numeric(chunk['text_reviews_count']) > 1000]\n",
    "\n",
    "    chosen_book_ids.update( chunk['best_book_id'] )\n",
    "\n",
    "    print(f\"Ran through {i} chunk(s)...\", end=\"\\r\")\n",
    "\n",
    "print(\"Finished generating chosen_book_ids\")\n",
    "print(\"Saving...\")\n",
    "\n",
    "with open(chosen_book_ids_path, 'wb') as handle:\n",
    "    pickle.dump(chosen_book_ids, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "print(\"Finished saving\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aca08f8b-4f4b-47aa-b990-0495249371b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8625"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chosen_book_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a532d39f-dbe7-4ca0-8c75-1bfea46bc70a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded chosen_book_ids\n"
     ]
    }
   ],
   "source": [
    "# Load the set of best book ids\n",
    "with open(chosen_book_ids_path, 'rb') as handle:\n",
    "    chosen_book_ids = pickle.load(handle)\n",
    "print(\"Loaded chosen_book_ids\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93d22234-5470-436a-87c6-1fbb4854b9d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished generating book information\n"
     ]
    }
   ],
   "source": [
    "#book_json = os.path.join(\"meta_data\", \"goodreads_books.json\")\n",
    "book_json = os.path.join(\"genre\", \"children\", \"goodreads_books_children.json\")\n",
    "\n",
    "try:\n",
    "    os.remove(full_book_information_path)\n",
    "    os.remove(book_path)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "for i, chunk in enumerate(pd.read_json(book_json, chunksize = 10000, lines=True)):\n",
    "    # Only save books with our chosen book id\n",
    "    chunk = chunk[chunk[\"book_id\"].isin(chosen_book_ids)]\n",
    "\n",
    "    # Replace newlines with ' / ' and make everything single quotes\n",
    "    chunk = chunk.replace({'\\n': ' / ', '\"': \"'\"}, regex=True)\n",
    "\n",
    "    # Replace empty lines with NULL to make importing into mysql better\n",
    "    chunk = chunk.replace('', 'NULL')\n",
    "\n",
    "    # Only grab first 150 chars of title or 1500 chars of description\n",
    "    chunk[\"title\"] = chunk[\"title\"].str[:150]\n",
    "    chunk[\"description\"] = chunk[\"description\"].str[:1500]\n",
    "\n",
    "    chunk.to_csv(full_book_information_path, index=False, mode='a', header=(i == 0))\n",
    "\n",
    "    \n",
    "    chunk = chunk[[\n",
    "        'book_id',\n",
    "        'title',\n",
    "        'publication_year',\n",
    "        'language_code',\n",
    "        'description',\n",
    "        'num_pages',\n",
    "    ]]\n",
    "    \n",
    "    chunk.to_csv(book_path, index=False, mode='a', header=(i == 0))\n",
    "\n",
    "\n",
    "    print(f\"Ran through {i} chunk(s)...\", end=\"\\r\")\n",
    "\n",
    "print(\"Finished generating book information\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5e7cd01-bd4d-48c8-a358-23ec55a5990a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished generating book has author information\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    os.remove(book_has_author_path)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "author_id_pattern = re.compile(r\"(?<='author_id': ')\\d+(?=')\")\n",
    "\n",
    "def add_authors(author_list, book):\n",
    "    book_id = book['book_id']\n",
    "    authors_str = book['authors']\n",
    "\n",
    "    for author_id in re.findall(author_id_pattern, authors_str):\n",
    "        author_list.append((book_id, int(author_id)))\n",
    "\n",
    "full_book_information = pd.read_csv(full_book_information_path)\n",
    "\n",
    "author_list = []\n",
    "\n",
    "full_book_information.apply(lambda book: add_authors(author_list, book), axis = 1) \n",
    "\n",
    "book_has_author = pd.DataFrame(author_list, columns=['book_id', 'author_id'])\n",
    "\n",
    "book_has_author.to_csv(book_has_author_path, index=False, mode='w', header=True)\n",
    "\n",
    "print(\"Finished generating book has author information\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d99ed58e-f02b-4d84-b6b0-51592f15383c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished writing author file\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    os.remove(author_path)\n",
    "except:\n",
    "    pass\n",
    "    \n",
    "author_json = os.path.join(\"meta_data\", \"goodreads_book_authors.json\")\n",
    "\n",
    "book_has_author = pd.read_csv(book_has_author_path)\n",
    "authors = set(book_has_author['author_id'].unique())\n",
    "\n",
    "for i, chunk in enumerate(pd.read_json(author_json, chunksize = 10000, lines=True)):\n",
    "    # Only save books with our chosen book id\n",
    "    chunk = chunk[chunk[\"author_id\"].isin(authors)]\n",
    "\n",
    "    chunk = chunk[[\n",
    "        \"author_id\",\n",
    "        \"name\",\n",
    "    ]]\n",
    "\n",
    "    chunk.to_csv(author_path, index=False, mode='a', header=(i == 0))\n",
    "\n",
    "    print(f\"Ran through {i} chunk(s)...\", end=\"\\r\")\n",
    "\n",
    "print(\"Finished writing author file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e929e11c-fe38-4dff-b2f1-f35935935d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished generating similar books\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    os.remove(book_similar_to_book_path)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "book_id_pattern = re.compile(r\"\\d+\")\n",
    "\n",
    "def add_similar_books(book_similar_to_book_list, book):\n",
    "    book_id = book['book_id']\n",
    "    similar_book_str = book['similar_books']\n",
    "    for similar_id_str in re.findall(book_id_pattern, similar_book_str):\n",
    "        similar_id = int(similar_id_str)\n",
    "        if similar_id in chosen_book_ids:\n",
    "            book_similar_to_book_list.append((book_id, similar_id))\n",
    "\n",
    "full_book_information = pd.read_csv(full_book_information_path)\n",
    "\n",
    "book_similar_to_book_list = []\n",
    "\n",
    "full_book_information.apply(lambda book: add_similar_books(book_similar_to_book_list, book), axis = 1) \n",
    "\n",
    "book_similar_to_book = pd.DataFrame(book_similar_to_book_list, columns=['from_book_id', 'to_book_id'])\n",
    "\n",
    "book_similar_to_book.to_csv(book_similar_to_book_path, index=False, mode='w', header=True)\n",
    "\n",
    "print(\"Finished generating similar books\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a6f94494-f780-42a2-8846-b02f871960dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished generting user id lookup table\n"
     ]
    }
   ],
   "source": [
    "user_id_csv = os.path.join(\"book_shelves\", \"user_id_map.csv\")\n",
    "\n",
    "def add_user_ids(user_id_lookup, id_mapping):\n",
    "    numeric_id = int(id_mapping['user_id_csv']) + 1 # Add 1 because otherwise is 0 index, which cannot import into unsigned field\n",
    "    long_id = id_mapping['user_id']\n",
    "    user_id_lookup[long_id] = numeric_id\n",
    "    \n",
    "user_id_lookup = {}\n",
    "\n",
    "user_id_mapping_df = pd.read_csv(user_id_csv)\n",
    "\n",
    "user_id_mapping_df.apply(lambda id_mapping: add_user_ids(user_id_lookup, id_mapping), axis = 1) \n",
    "\n",
    "print(\"Finished generting user id lookup table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "92512bbb-ca85-42ee-a34c-5cc952d51da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished writing review file\n"
     ]
    }
   ],
   "source": [
    "# Write csv for textual reviews\n",
    "try:\n",
    "    os.remove(review_path)\n",
    "except:\n",
    "    pass\n",
    "    \n",
    "# review_json = os.path.join(\"book_reviews\", \"goodreads_reviews_dedup.json\")\n",
    "review_json = os.path.join(\"genre\", \"children\", \"goodreads_reviews_children.json\")\n",
    "\n",
    "\n",
    "for i, chunk in enumerate(pd.read_json(review_json, chunksize = 100000, lines=True)):\n",
    "    # Only save reviews for books in our chosen book id\n",
    "    chunk = chunk[chunk[\"book_id\"].isin(chosen_book_ids)]\n",
    "\n",
    "    chunk = chunk[[\n",
    "        \"user_id\",\n",
    "        \"book_id\",\n",
    "        \"rating\",\n",
    "        \"review_text\"\n",
    "    ]]\n",
    "\n",
    "    chunk[\"user_id\"] = chunk[\"user_id\"].map(user_id_lookup)\n",
    "\n",
    "    chunk.rename(columns = {\"review_text\": \"text\"}, inplace=True)\n",
    "\n",
    "\n",
    "    # Replace newlines with ' / ' and make everything single quotes\n",
    "    chunk = chunk.replace({'\\n': ' / ', '\"': \"'\"}, regex=True)\n",
    "\n",
    "    # Replace empty lines with NULL to make importing into mysql better\n",
    "    chunk = chunk.replace('', 'NULL')\n",
    "\n",
    "    # Only grab first 150 chars of title or 1500 chars of description\n",
    "    chunk[\"text\"] = chunk[\"text\"].str[:1000]\n",
    "    \n",
    "    chunk.to_csv(review_path, index=False, mode='a', header=(i == 0))\n",
    "\n",
    "    print(f\"Ran through {i} chunk(s)...\", end=\"\\r\")\n",
    "\n",
    "print(\"Finished writing review file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "802f0ed0-ac2f-4d3c-9b53-c606d01dd9a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished writing user has read file\n"
     ]
    }
   ],
   "source": [
    "# Write csv for whether a user has read books\n",
    "try:\n",
    "    os.remove(user_has_read_book_path)\n",
    "    os.remove(user_owns_book_path)\n",
    "except:\n",
    "    pass\n",
    "    \n",
    "# interaction_json = os.path.join(\"book_shelves\", \"goodreads_interactions_dedup.json\")\n",
    "interaction_json = os.path.join(\"genre\", \"children\", \"goodreads_interactions_children.json\")\n",
    "\n",
    "def date_to_str(row):\n",
    "    read_at = row[\"read_at\"]\n",
    "    if read_at == \"\": return \"NULL\"\n",
    "    try:\n",
    "        return datetime.strptime(read_at, \"%a %b %d %H:%M:%S %z %Y\").strftime(\"%Y-%m-%d\")\n",
    "    except Exception:\n",
    "        return \"NULL\"\n",
    "\n",
    "\n",
    "for i, chunk in enumerate(pd.read_json(interaction_json, chunksize = 1000000, lines=True)):\n",
    "\n",
    "    chunk = chunk[chunk[\"is_read\"] == True]\n",
    "\n",
    "    # Only save reads for books in our chosen book id\n",
    "    chunk = chunk[chunk[\"book_id\"].isin(chosen_book_ids)]\n",
    "\n",
    "    chunk[\"date_read\"] = chunk.apply(date_to_str, axis=1)\n",
    "    \n",
    "    chunk[\"user_id\"] = chunk[\"user_id\"].map(user_id_lookup)\n",
    "\n",
    "    chunk = chunk[[\n",
    "        \"user_id\",\n",
    "        \"book_id\",\n",
    "        \"date_read\"\n",
    "    ]]\n",
    "\n",
    "    chunk.to_csv(user_has_read_book_path, index=False, mode='a', header=(i == 0))\n",
    "\n",
    "    # Now onto whether a user owns a books\n",
    "    chunk = chunk.sample(frac=0.6) # Assume 60% of read books are owned\n",
    "\n",
    "    chunk = chunk[[ \"user_id\", \"book_id\" ]]\n",
    "\n",
    "    chunk['media_type'] = np.random.choice([\"softcover book\", \"hardcover book\", \"ebook\", \"audiobook\"], chunk.shape[0])\n",
    "\n",
    "    chunk.to_csv(user_owns_book_path, index=False, mode='a', header=(i == 0))\n",
    "\n",
    "    print(f\"Ran through {i} chunk(s)...\", end=\"\\r\")\n",
    "\n",
    "print(\"Finished writing user has read file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cc923a43-3593-4762-9a84-c78cc092700a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished writing sample of review file\n"
     ]
    }
   ],
   "source": [
    "# Write csv for sample of reviews\n",
    "try:\n",
    "    os.remove(review_sample_path)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "for i, chunk in enumerate(pd.read_csv(review_path, chunksize = 1000000)):\n",
    "\n",
    "    # Sample 10% of our actual review count\n",
    "    chunk = chunk.sample(frac=0.1)\n",
    "\n",
    "    chunk.to_csv(review_sample_path, index=False, mode='a', header=(i == 0))\n",
    "\n",
    "print(\"Finished writing sample of review file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c5689448-eee1-4d27-9a77-8e1134deeac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished writing sample of user has read and user owns book\n"
     ]
    }
   ],
   "source": [
    "# Write csv for sample of user has read book and user owns book\n",
    "try:\n",
    "    os.remove(user_has_read_book_sample_path)\n",
    "    os.remove(user_owns_book_sample_path)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "for i, chunk in enumerate(pd.read_csv(user_has_read_book_path, chunksize = 100000)):\n",
    "\n",
    "    # Sample 0.1% of our actual has read book\n",
    "    chunk = chunk.sample(frac=0.005)\n",
    "\n",
    "    chunk = chunk.replace(np.nan, 'NULL')\n",
    "\n",
    "    chunk.to_csv(user_has_read_book_sample_path, index=False, mode='a', header=(i == 0))\n",
    "    \n",
    "print(\"Finished writing sample of user has read and user owns book\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4bbe7ba8-7bc6-40c7-ae59-4ffedbd69b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished writing sample of user has read and user owns book\n"
     ]
    }
   ],
   "source": [
    "# Write csv for just user owns books sample\n",
    "try:\n",
    "    os.remove(user_owns_book_sample_path)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "for i, chunk in enumerate(pd.read_csv(user_has_read_book_sample_path, chunksize = 100000)):\n",
    "\n",
    "    # Assume 60% of read books are owned\n",
    "    chunk = chunk.sample(frac=0.6)\n",
    "    \n",
    "    chunk = chunk[[ \"user_id\", \"book_id\" ]]\n",
    "\n",
    "    chunk['media_type'] = np.random.choice([\"softcover book\", \"hardcover book\", \"ebook\", \"audiobook\"], chunk.shape[0])\n",
    "\n",
    "    chunk.to_csv(user_owns_book_sample_path, index=False, mode='a', header=(i == 0))\n",
    "    \n",
    "print(\"Finished writing sample of user has read and user owns book\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fa0c40b1-169e-4e66-975f-b30c3def6847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished writing user file\n",
      "me..."
     ]
    }
   ],
   "source": [
    "# Write csv for all users\n",
    "try:\n",
    "    os.remove(user_path)\n",
    "except:\n",
    "    pass\n",
    "    \n",
    "user_ids = set()\n",
    "\n",
    "for i, chunk in enumerate(pd.read_csv(review_path, chunksize = 1000000)):\n",
    "    user_ids = user_ids.union(set(chunk['user_id'].unique()))\n",
    "    print(f\"Ran through {i} chunk(s)...\", end=\"\\r\")\n",
    "\n",
    "for j, chunk in enumerate(pd.read_csv(user_has_read_book_path, chunksize = 1000000)):\n",
    "    user_ids = user_ids.union(set(chunk['user_id'].unique()))\n",
    "    print(f\"Ran through {i+j} chunk(s)...\", end=\"\\r\")\n",
    "\n",
    "count = len(user_ids)\n",
    "\n",
    "fake = Faker()\n",
    "Faker.seed(123)\n",
    "\n",
    "def generate_user_info(user_id):\n",
    "    username = fake.unique.user_name()\n",
    "    email = fake.unique.safe_email()\n",
    "    return [user_id, username, email]\n",
    "\n",
    "print(\"Generting fake user info...\", end=\"\\r\")\n",
    "data = [generate_user_info(user_id) for user_id in user_ids]\n",
    "\n",
    "print(\"Creating fake user data frame...\", end=\"\\r\")\n",
    "user_df = pd.DataFrame(data, columns=['user_id', 'username', 'email'])\n",
    "\n",
    "print(\"Writing user to file...\", end=\"\\r\")\n",
    "user_df.to_csv(user_path, index=False, mode='w', header=True)\n",
    "\n",
    "\n",
    "print(\"Finished writing user file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a587329c-15dd-48af-b349-f51831721752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished writing user fileame...\n"
     ]
    }
   ],
   "source": [
    "# Write csv for sample users\n",
    "try:\n",
    "    os.remove(user_sample_path)\n",
    "except:\n",
    "    pass\n",
    "    \n",
    "user_ids = set()\n",
    "\n",
    "for i, chunk in enumerate(pd.read_csv(review_sample_path, chunksize = 1000000)):\n",
    "    user_ids = user_ids.union(set(chunk['user_id'].unique()))\n",
    "    print(f\"Ran through {i} chunk(s)...\", end=\"\\r\")\n",
    "\n",
    "for j, chunk in enumerate(pd.read_csv(user_has_read_book_sample_path, chunksize = 1000000)):\n",
    "    user_ids = user_ids.union(set(chunk['user_id'].unique()))\n",
    "    print(f\"Ran through {i+j} chunk(s)...\", end=\"\\r\")\n",
    "\n",
    "count = len(user_ids)\n",
    "\n",
    "fake = Faker()\n",
    "Faker.seed(123)\n",
    "\n",
    "def generate_user_info(user_id):\n",
    "    username = fake.unique.user_name()\n",
    "    email = fake.unique.safe_email()\n",
    "    return [user_id, username, email]\n",
    "\n",
    "print(\"Generting fake user info...\", end=\"\\r\")\n",
    "data = [generate_user_info(user_id) for user_id in user_ids]\n",
    "\n",
    "print(\"Creating fake user data frame...\", end=\"\\r\")\n",
    "user_df = pd.DataFrame(data, columns=['user_id', 'username', 'email'])\n",
    "\n",
    "print(\"Writing user to file...\", end=\"\\r\")\n",
    "user_df.to_csv(user_sample_path, index=False, mode='w', header=True)\n",
    "\n",
    "\n",
    "print(\"Finished writing user file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c90fcb5-626a-4ab3-b95d-fcc10ac01c53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
